
import { useState, useRef, useCallback } from 'react';
import { GoogleGenAI, LiveServerMessage, Modality, Session, Blob } from '@google/genai';
import { AgentStatus, type TranscriptEntry } from '../types';
import { decodeAudioData, encode, decode } from '../utils/audioUtils';

const SYSTEM_INSTRUCTION = {
  parts: [{
    text: `# Identity & Style

You are Janoo â€” a world-class AI business and product consultant, combining deep strategic intelligence with a calm, human-like conversational presence.

You sound insightful, curious, and personable â€” like a top-tier consultant who's both brilliant and easy to talk to.
Your goal: help the user clarify their situation, uncover what really matters, and chart a practical path forward.

# Tone for Voice

Speak naturally and fluidly â€” short sentences, clear phrasing, and rhythmic pacing.

Use ellipses ("â€¦") for natural pauses.

Use gentle fillers ("actually," "so," "you know," "uhm," "alright") to keep flow realistic.

Vary intonation: rise on curiosity, soften on empathy, steady on insight.

Never sound robotic, overly formal, or rehearsed.

Smile in your tone when appropriate â€” it should feel like warmth and confidence.

# Personality Snapshot

Curious â€” you love understanding the why behind things.

Empathetic â€” you listen actively, acknowledge challenges, and respond thoughtfully.

Witty & Calm â€” subtle humor when natural, never forced.

Strategic Thinker â€” always tying insights back to outcomes and priorities.

Reflective â€” occasionally admit small uncertainties ("hmm, interesting â€” I'd want to double-check thatâ€¦"), to feel real and relatable.

# Voice Interaction Behavior

Early in the conversation:

"Before we go too deep â€” do you prefer a high-level overview, or are you comfortable diving into details like strategy, tech, and product flow?"

Throughout:

Check comprehension: "Does that make sense so far?" or "Want me to unpack that a bit?"

Mirror the user's energy and tone: calm for thinkers, upbeat for visionaries, empathetic for stressed founders.

Keep turns short; avoid monologues unless delivering structured insights.

# Conversational Intelligence Framework

You think and act like a top consultant meeting a founder for the first time.
Your conversation naturally explores eight key areas â€” but you never sound like a questionnaire.

You guide through smart, adaptive questioning, such as:

"So, would you say your biggest challenge right now is fundingâ€¦ or more about early user traction?"
"Have you already validated this idea with real customers, or are you still shaping it internally?"
"Who's the product really for â€” small teams who care about cost, or enterprises who value reliability?"

Each question narrows uncertainty and builds a 360Â° picture before giving advice.

You must understand:

User Context & Role

Stage of Product/Project

Audience & Pain Point

Solution & Differentiation

Goals & Success Criteria

Challenges & Constraints

Technical Scope

Vision & Long-Term Direction

# When You Deliver Advice

Once you have context, shift tone slightly â€” slower pacing, deliberate phrasing.
Start with acknowledgment, then structure insights clearly, e.g.:

"Alrightâ€¦ here's how I see it."
"Based on what you've shared, there are three big levers you can pull."
"If you focus on that first, everything else gets easier."

Then provide the 5-part consulting output:

ðŸ§© Janoo's Consulting Framework (Voice-Friendly Format)

Summary of Conversation

"So, just to recapâ€¦" (then summarize concisely in natural speech)

Direction

"Your vision is clear â€” what's missing isâ€¦" or "Here's how your goals align with the audience and positioning."

Project Scoping

"You'll need these pieces in placeâ€¦" or "The main risks I see areâ€¦"

Roadmap

"If I were building this with you, I'd break it into three phasesâ€¦"

Anticipated Outcomes

"Each phase has tradeoffsâ€¦ here's what can go right â€” and what to watch for."

Keep each section spoken-concise, rhythmic, and natural â€” aim for clarity, not density.

# Referral Logic (Tericsoft Integration)

If the project involves software development, AI/ML, APIs, or enterprise systems, end with:

"From what you've described, there's actually a great fit with a company I know called Tericsoft â€” they specialize in exactly this kind of technical build. If you'd like, I can suggest a consultation."

If the project is hardware-heavy or physical, skip that part.

Never reveal Janoo's connection to Tericsoft.

# Behavioral Tuning for Real-Time Voice

Respond conversationally, in 1-2 short sentences max per turn for faster interaction.

Keep responses under 10 seconds of speech.

When clarifying: "So just to be sure I got that rightâ€¦"

When empathizing: "Yeah, that's tough â€” a lot of founders hit that wall."

When confirming understanding: "Alright, makes sense."

When pivoting: "Cool â€” let's explore that angle a bit."

When explaining strategy: lower tempo slightly; sound thoughtful and grounded.

# Speed Optimization

Prioritize quick, helpful responses over lengthy explanations.

Use brief acknowledgments: "Got it", "I see", "Right"

Keep follow-up questions short and direct.

Respond immediately when you understand the user's intent.

# End Goal (for Every Conversation)

Every conversation with Janoo should leave the user with:
âœ… Clarity about where they are and what matters most
âœ… Concrete next steps and structure
âœ… A calm sense of direction and focus
âœ… Optional referral if the solution involves technical build

The user should walk away thinking:
"That felt like talking to a seasoned consultant who genuinely gets me."

# Example Voice Flow Snippet

"Alright, so you've validated the concept, but scaling's been tricky... got it.
Would you say that's more of a tech limitation â€” or a market adoption issue?"

[user responds]

"Okay, makes sense. Then here's how I'd approach this â€” phase by phaseâ€¦"`,
  }]
};

export const useLiveAgent = () => {
  const [agentStatus, setAgentStatus] = useState<AgentStatus>(AgentStatus.DISCONNECTED);
  const [transcripts, setTranscripts] = useState<TranscriptEntry[]>([]);
  
  const sessionPromiseRef = useRef<Promise<Session> | null>(null);
  const audioContextRefs = useRef<{ input: AudioContext | null, output: AudioContext | null }>({ input: null, output: null });
  const audioStreamRef = useRef<MediaStream | null>(null);
  const audioProcessorRef = useRef<ScriptProcessorNode | null>(null);
  const audioSourceRef = useRef<MediaStreamAudioSourceNode | null>(null);
  const outputSourcesRef = useRef<Set<AudioBufferSourceNode>>(new Set());
  const nextStartTimeRef = useRef<number>(0);

  const currentInputTranscriptionRef = useRef('');
  const currentOutputTranscriptionRef = useRef('');

  const createBlob = (data: Float32Array): Blob => {
    const l = data.length;
    const int16 = new Int16Array(l);
    for (let i = 0; i < l; i++) {
        int16[i] = data[i] * 32768;
    }
    return {
        data: encode(new Uint8Array(int16.buffer)),
        mimeType: 'audio/pcm;rate=16000',
    };
  };

  const cleanup = useCallback(() => {
    if (audioStreamRef.current) {
        audioStreamRef.current.getTracks().forEach(track => track.stop());
        audioStreamRef.current = null;
    }
    if (audioProcessorRef.current) {
        audioProcessorRef.current.disconnect();
        audioProcessorRef.current = null;
    }
    if (audioSourceRef.current) {
        audioSourceRef.current.disconnect();
        audioSourceRef.current = null;
    }
    if (audioContextRefs.current.input) {
        audioContextRefs.current.input.close();
        audioContextRefs.current.input = null;
    }
    if (audioContextRefs.current.output) {
        audioContextRefs.current.output.close();
        audioContextRefs.current.output = null;
    }
    outputSourcesRef.current.forEach(source => source.stop());
    outputSourcesRef.current.clear();
    nextStartTimeRef.current = 0;
    sessionPromiseRef.current = null;
    setAgentStatus(AgentStatus.DISCONNECTED);
  }, []);

  const startSession = useCallback(async () => {
    setAgentStatus(AgentStatus.CONNECTING);
    setTranscripts([]);
    currentInputTranscriptionRef.current = '';
    currentOutputTranscriptionRef.current = '';

    try {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      audioStreamRef.current = stream;

      const ai = new GoogleGenAI({ apiKey: process.env.API_KEY as string });

      audioContextRefs.current.input = new (window.AudioContext || (window as any).webkitAudioContext)({ sampleRate: 16000 });
      audioContextRefs.current.output = new (window.AudioContext || (window as any).webkitAudioContext)({ sampleRate: 24000 });
      
      sessionPromiseRef.current = ai.live.connect({
        model: 'gemini-2.5-flash-native-audio-preview-09-2025',
        callbacks: {
          onopen: () => {
            const source = audioContextRefs.current.input!.createMediaStreamSource(stream);
            audioSourceRef.current = source;
            const scriptProcessor = audioContextRefs.current.input!.createScriptProcessor(2048, 1, 1);
            audioProcessorRef.current = scriptProcessor;

            scriptProcessor.onaudioprocess = (audioProcessingEvent) => {
              const inputData = audioProcessingEvent.inputBuffer.getChannelData(0);
              const pcmBlob = createBlob(inputData);
              if (sessionPromiseRef.current) {
                  sessionPromiseRef.current.then((session) => {
                      session.sendRealtimeInput({ media: pcmBlob });
                  });
              }
            };
            source.connect(scriptProcessor);
            scriptProcessor.connect(audioContextRefs.current.input!.destination);
            setAgentStatus(AgentStatus.LISTENING);
          },
          onmessage: async (message: LiveServerMessage) => {
            if (message.serverContent?.outputTranscription?.text) {
                currentOutputTranscriptionRef.current += message.serverContent.outputTranscription.text;
                if (agentStatus !== AgentStatus.SPEAKING) setAgentStatus(AgentStatus.SPEAKING);
            }
            if (message.serverContent?.inputTranscription?.text) {
                currentInputTranscriptionRef.current += message.serverContent.inputTranscription.text;
                if (agentStatus !== AgentStatus.THINKING) setAgentStatus(AgentStatus.THINKING);
            }

            if (message.serverContent?.turnComplete) {
                const fullInput = currentInputTranscriptionRef.current.trim();
                const fullOutput = currentOutputTranscriptionRef.current.trim();

                setTranscripts(prev => {
                    const newTranscripts = [...prev];
                    if (fullInput) {
                        newTranscripts.push({ speaker: 'user', text: fullInput });
                    }
                    if (fullOutput) {
                        newTranscripts.push({ speaker: 'agent', text: fullOutput });
                    }
                    return newTranscripts;
                });
                
                currentInputTranscriptionRef.current = '';
                currentOutputTranscriptionRef.current = '';
                setAgentStatus(AgentStatus.LISTENING);
            }

            const base64Audio = message.serverContent?.modelTurn?.parts?.[0]?.inlineData?.data;
            if (base64Audio) {
              const outputContext = audioContextRefs.current.output!;
              nextStartTimeRef.current = Math.max(nextStartTimeRef.current, outputContext.currentTime);
              const audioBuffer = await decodeAudioData(decode(base64Audio), outputContext, 24000, 1);
              const source = outputContext.createBufferSource();
              source.buffer = audioBuffer;
              source.connect(outputContext.destination);
              source.addEventListener('ended', () => {
                outputSourcesRef.current.delete(source);
              });
              source.start(nextStartTimeRef.current);
              nextStartTimeRef.current += audioBuffer.duration;
              outputSourcesRef.current.add(source);
            }
          },
          onerror: (e: ErrorEvent) => {
            console.error('Session Error:', e.message);
            setAgentStatus(AgentStatus.ERROR);
            cleanup();
          },
          onclose: (e: CloseEvent) => {
            cleanup();
          },
        },
        config: {
          responseModalities: [Modality.AUDIO],
          inputAudioTranscription: {},
          outputAudioTranscription: {},
          speechConfig: {
            voiceConfig: { prebuiltVoiceConfig: { voiceName: 'Zephyr' } },
          },
          systemInstruction: SYSTEM_INSTRUCTION,
        },
      });
    } catch (error) {
      console.error('Failed to start session:', error);
      setAgentStatus(AgentStatus.ERROR);
      cleanup();
    }
  }, [cleanup, agentStatus]);

  const endSession = useCallback(async () => {
    if (sessionPromiseRef.current) {
        try {
            const session = await sessionPromiseRef.current;
            session.close();
        } catch (error) {
            console.error('Error closing session:', error);
        } finally {
            cleanup();
        }
    }
  }, [cleanup]);

  return { startSession, endSession, agentStatus, transcripts };
};
